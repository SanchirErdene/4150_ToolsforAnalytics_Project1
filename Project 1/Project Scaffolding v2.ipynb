{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in c:\\users\\sanch\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.22 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from geopandas) (1.26.4)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from geopandas) (0.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from geopandas) (23.2)\n",
      "Requirement already satisfied: pandas>=1.4.0 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from geopandas) (2.2.2)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from geopandas) (3.7.0)\n",
      "Requirement already satisfied: shapely>=2.0.0 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from geopandas) (2.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from pandas>=1.4.0->geopandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from pandas>=1.4.0->geopandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from pandas>=1.4.0->geopandas) (2023.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from pyogrio>=0.7.2->geopandas) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas) (1.16.0)\n",
      "Requirement already satisfied: fiona in c:\\users\\sanch\\anaconda3\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: shapely in c:\\users\\sanch\\anaconda3\\lib\\site-packages (2.0.6)\n",
      "Requirement already satisfied: pyproj in c:\\users\\sanch\\anaconda3\\lib\\site-packages (3.7.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from fiona) (23.1.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from fiona) (2024.8.30)\n",
      "Requirement already satisfied: click~=8.0 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from fiona) (8.1.7)\n",
      "Requirement already satisfied: click-plugins>=1.0 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from fiona) (1.1.1)\n",
      "Requirement already satisfied: cligj>=0.5 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from fiona) (0.7.2)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from shapely) (1.26.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from click~=8.0->fiona) (0.4.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\sanch\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\sanch\\anaconda3\\lib\\site-packages (14.0.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sanch\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "!pip install geopandas\n",
    "import geopandas as gpd\n",
    "!pip install fiona shapely pyproj\n",
    "import math\n",
    "!pip install pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = r\"C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = r\"C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\weather_data\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0783b90-440b-4c24-8996-c4b9ee2fa3f7",
   "metadata": {},
   "source": [
    "### Download Yellow Taxi and HVFHV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "287d708f-460d-440e-8e23-79033c7e6867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_html() -> str:\n",
    "    \"\"\"Fetch the HTML content of the taxi data page.\"\"\"\n",
    "    response = requests.get(TAXI_URL)\n",
    "    response.raise_for_status()\n",
    "    html = response.content\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3318cfa8-09dd-486e-917c-f261d63c7697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6beaf1c-6288-43c8-9b9a-3ec29d000c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f6a172-b332-40a3-bad2-7088741f4718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile):\n",
    "    \"\"\"\n",
    "    Load the Taxi Zones shapefile and return a GeoDataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    shapefile (str): The path to the Taxi Zones shapefile.\n",
    "\n",
    "    Returns:\n",
    "    geopandas.GeoDataFrame: A GeoDataFrame containing the Taxi Zones data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        taxi_zones = gpd.read_file(shapefile)\n",
    "        print(f\"Taxi Zones data loaded successfully. Total zones: {len(taxi_zones)}\")\n",
    "        return taxi_zones\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Taxi Zones shapefile: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d41a3b1a-b6e1-4b80-801c-3f634fcbe5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi Zones data loaded successfully. Total zones: 263\n",
      "   OBJECTID  Shape_Leng  Shape_Area                     zone  LocationID  \\\n",
      "0         1    0.116357    0.000782           Newark Airport           1   \n",
      "1         2    0.433470    0.004866              Jamaica Bay           2   \n",
      "2         3    0.084341    0.000314  Allerton/Pelham Gardens           3   \n",
      "3         4    0.043567    0.000112            Alphabet City           4   \n",
      "4         5    0.092146    0.000498            Arden Heights           5   \n",
      "\n",
      "         borough                                           geometry  \n",
      "0            EWR  POLYGON ((933100.918 192536.086, 933091.011 19...  \n",
      "1         Queens  MULTIPOLYGON (((1033269.244 172126.008, 103343...  \n",
      "2          Bronx  POLYGON ((1026308.77 256767.698, 1026495.593 2...  \n",
      "3      Manhattan  POLYGON ((992073.467 203714.076, 992068.667 20...  \n",
      "4  Staten Island  POLYGON ((935843.31 144283.336, 936046.565 144...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the shapefile\n",
    "taxi_zones = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "\n",
    "# Inspect the loaded data\n",
    "if taxi_zones is not None:\n",
    "    print(taxi_zones.head())  # Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1f21ff5-1a77-414a-9888-4de4718d9666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73\n",
      "  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
      "  92  93  94  95  96  97  98  99 100 101 102 103 106 107 108 109 110 111\n",
      " 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129\n",
      " 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147\n",
      " 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165\n",
      " 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183\n",
      " 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201\n",
      " 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219\n",
      " 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237\n",
      " 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255\n",
      " 256 257 258 259 260 261 262 263]\n",
      "Total unique Location_IDs: 260\n"
     ]
    }
   ],
   "source": [
    "unique_location_ids = taxi_zones['LocationID'].unique()\n",
    "\n",
    "# Display the unique Location_ID values\n",
    "print(unique_location_ids)\n",
    "print(f\"Total unique Location_IDs: {len(unique_location_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced7809-14dd-4489-9112-1c5e8c4ba2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones):\n",
    "\n",
    "    \"\"\"\n",
    "    Look up the latitude and longitude for a given Taxi Zone location ID.\n",
    "\n",
    "    Parameters:\n",
    "    - zone_loc_id (int): The Taxi Zone Location ID.\n",
    "    - loaded_taxi_zones (GeoDataFrame): The GeoDataFrame containing taxi zones.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (latitude, longitude) of the zone's centroid, or None if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the GeoDataFrame is in a geographic CRS (EPSG:4326)\n",
    "        loaded_taxi_zones = loaded_taxi_zones.to_crs(epsg=4326)\n",
    "        \n",
    "        # Find the row corresponding to the specified location ID\n",
    "        zone = loaded_taxi_zones.loc[loaded_taxi_zones['LocationID'] == zone_loc_id]\n",
    "        \n",
    "        if zone.empty:\n",
    "            print(f\"location ID {zone_loc_id} not found.\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate centroid and extract coordinates\n",
    "        lon = zone.geometry.centroid.x.values[0]\n",
    "        lat = zone.geometry.centroid.y.values[0]\n",
    "        \n",
    "        return lat, lon\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while looking up coordinates: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7046d13c-842a-4102-a36f-d28469d447be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi Zones data loaded successfully. Total zones: 263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanch\\AppData\\Local\\Temp\\ipykernel_15932\\3578696659.py:25: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  lon = zone.geometry.centroid.x.values[0]\n",
      "C:\\Users\\sanch\\AppData\\Local\\Temp\\ipykernel_15932\\3578696659.py:26: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  lat = zone.geometry.centroid.y.values[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location ID 57 not found.\n",
      "location ID 104 not found.\n",
      "location ID 105 not found.\n",
      "     LocationID   Latitude  Longitude\n",
      "0             1  40.691831 -74.174000\n",
      "1             2  40.616745 -73.831299\n",
      "2             3  40.864474 -73.847422\n",
      "3             4  40.723752 -73.976968\n",
      "4             5  40.552659 -74.188484\n",
      "..          ...        ...        ...\n",
      "255         259  40.897932 -73.852215\n",
      "256         260  40.744235 -73.906306\n",
      "257         261  40.709139 -74.013023\n",
      "258         262  40.775932 -73.946510\n",
      "259         263  40.778766 -73.951010\n",
      "\n",
      "[260 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "taxi_zones = load_taxi_zones(TAXI_ZONES_SHAPEFILE) # Same as above \n",
    " \n",
    "coords_list = []\n",
    "\n",
    "# Loop through LocationID from 1 to 263\n",
    "for zone_id in range(1, 264):\n",
    "    coords = lookup_coords_for_taxi_zone_id(zone_id, taxi_zones)\n",
    "    if coords:\n",
    "        coords_list.append({'LocationID': zone_id, 'Latitude': coords[0], 'Longitude': coords[1]})\n",
    "\n",
    "# Convert the list of results into a DataFrame\n",
    "coords_df = pd.DataFrame(coords_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(coords_df)\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "coords_df.to_csv(\"taxi_zone_coordinates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0adf15-c290-4fa8-88d3-4762a2584fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_sample_size(population, confidence_level=0.95, margin_of_error=0.05, proportion=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the sample size based on Cochran's formula.\n",
    "    \n",
    "    Parameters:\n",
    "    - population (int): Total population size (i.e., total data points for each month)\n",
    "    - confidence_level (float): Confidence level (default is 0.95 for 95% confidence)\n",
    "    - margin_of_error (float): Margin of error (default is 0.05 for 5% margin of error)\n",
    "    - proportion (float): Estimated proportion of the population (default is 0.5 for maximum sample size)\n",
    "    \n",
    "    Returns:\n",
    "    - int: Required sample size\n",
    "    \"\"\"\n",
    "    # Z-scores for common confidence levels\n",
    "    z_scores = {\n",
    "        0.90: 1.645,\n",
    "        0.95: 1.96,\n",
    "        0.99: 2.576\n",
    "    }\n",
    "    \n",
    "    # Get the Z-score for the desired confidence level\n",
    "    Z = z_scores.get(confidence_level, 1.96)  # Default to 95% confidence if not found\n",
    "    \n",
    "    # Cochran's sample size formula (n0)\n",
    "    n0 = (Z ** 2 * proportion * (1 - proportion)) / (margin_of_error ** 2)\n",
    "    \n",
    "    # Adjust for finite population if necessary\n",
    "    if population < 1000:  # If population is small, apply finite population correction\n",
    "        n = n0 / (1 + (n0 - 1) / population)\n",
    "    else:\n",
    "        n = n0\n",
    "    \n",
    "    return math.ceil(n)  # Round up to ensure the sample size is sufficient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09667342-df3e-4878-91d4-07b2765ddae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a2f4cc69-765e-452c-ba2a-c8a5c70943f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population size:6405008\n",
      "Sample size for this month: 385\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Can Delete later: TESTING for SAMPLING only 2020.1 Yellow Taxi\n",
    "\n",
    "month_file = r\"C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\taxi_data\\yellow_tripdata_2020-01.parquet\"\n",
    "month_data = pd.read_parquet(month_file)\n",
    "\n",
    "# Get the population size (total records for the month)\n",
    "population_size = len(month_data)\n",
    "\n",
    "# Calculate the sample size\n",
    "sample_size = calculate_sample_size(population_size, confidence_level=0.95, margin_of_error=0.05)\n",
    "print(f\"population size:{population_size}\")\n",
    "print(f\"Sample size for this month: {sample_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "### Common Functions: Downloading the Yellow Taxi Parquet links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading yellow_tripdata_2024-01.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet...\n",
      "File yellow_tripdata_2024-01.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2024-02.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet...\n",
      "File yellow_tripdata_2024-02.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2024-03.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet...\n",
      "File yellow_tripdata_2024-03.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2024-04.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-04.parquet...\n",
      "File yellow_tripdata_2024-04.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2024-05.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-05.parquet...\n",
      "File yellow_tripdata_2024-05.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2024-06.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-06.parquet...\n",
      "File yellow_tripdata_2024-06.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2024-07.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-07.parquet...\n",
      "File yellow_tripdata_2024-07.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2024-08.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-08.parquet...\n",
      "File yellow_tripdata_2024-08.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2023-01.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet...\n",
      "File yellow_tripdata_2023-01.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2023-02.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet...\n",
      "File yellow_tripdata_2023-02.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2023-03.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet...\n",
      "File yellow_tripdata_2023-03.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2023-04.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-04.parquet...\n",
      "File yellow_tripdata_2023-04.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2023-05.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-05.parquet...\n",
      "File yellow_tripdata_2023-05.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2023-06.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-06.parquet...\n",
      "File yellow_tripdata_2023-06.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2023-07.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-07.parquet...\n",
      "File yellow_tripdata_2023-07.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2023-08.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-08.parquet...\n",
      "File yellow_tripdata_2023-08.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2023-09.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet...\n",
      "File yellow_tripdata_2023-09.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2023-10.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-10.parquet...\n",
      "File yellow_tripdata_2023-10.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2023-11.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-11.parquet...\n",
      "File yellow_tripdata_2023-11.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2023-12.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-12.parquet...\n",
      "File yellow_tripdata_2023-12.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2022-01.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet...\n",
      "File yellow_tripdata_2022-01.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2022-02.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet...\n",
      "File yellow_tripdata_2022-02.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2022-03.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-03.parquet...\n",
      "File yellow_tripdata_2022-03.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2022-04.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-04.parquet...\n",
      "File yellow_tripdata_2022-04.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2022-05.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-05.parquet...\n",
      "File yellow_tripdata_2022-05.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2022-06.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-06.parquet...\n",
      "File yellow_tripdata_2022-06.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2022-07.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-07.parquet...\n",
      "File yellow_tripdata_2022-07.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2022-08.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-08.parquet...\n",
      "File yellow_tripdata_2022-08.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2022-09.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-09.parquet...\n",
      "File yellow_tripdata_2022-09.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2022-10.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-10.parquet...\n",
      "File yellow_tripdata_2022-10.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2022-11.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-11.parquet...\n",
      "File yellow_tripdata_2022-11.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2022-12.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-12.parquet...\n",
      "File yellow_tripdata_2022-12.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2021-01.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet...\n",
      "File yellow_tripdata_2021-01.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2021-02.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet...\n",
      "File yellow_tripdata_2021-02.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2021-03.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-03.parquet...\n",
      "File yellow_tripdata_2021-03.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2021-04.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-04.parquet...\n",
      "File yellow_tripdata_2021-04.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2021-05.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-05.parquet...\n",
      "File yellow_tripdata_2021-05.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2021-06.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-06.parquet...\n",
      "File yellow_tripdata_2021-06.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2021-07.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-07.parquet...\n",
      "File yellow_tripdata_2021-07.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2021-08.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-08.parquet...\n",
      "File yellow_tripdata_2021-08.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2021-09.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-09.parquet...\n",
      "File yellow_tripdata_2021-09.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2021-10.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-10.parquet...\n",
      "File yellow_tripdata_2021-10.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2021-11.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-11.parquet...\n",
      "File yellow_tripdata_2021-11.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2021-12.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-12.parquet...\n",
      "File yellow_tripdata_2021-12.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2020-01.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-01.parquet...\n",
      "File yellow_tripdata_2020-01.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2020-02.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-02.parquet...\n",
      "File yellow_tripdata_2020-02.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2020-03.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-03.parquet...\n",
      "File yellow_tripdata_2020-03.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2020-04.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-04.parquet...\n",
      "File yellow_tripdata_2020-04.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2020-05.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-05.parquet...\n",
      "File yellow_tripdata_2020-05.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2020-06.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-06.parquet...\n",
      "File yellow_tripdata_2020-06.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2020-07.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-07.parquet...\n",
      "File yellow_tripdata_2020-07.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2020-08.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-08.parquet...\n",
      "File yellow_tripdata_2020-08.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2020-09.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-09.parquet...\n",
      "File yellow_tripdata_2020-09.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2020-10.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-10.parquet...\n",
      "File yellow_tripdata_2020-10.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2020-11.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-11.parquet...\n",
      "File yellow_tripdata_2020-11.parquet already exists. Skipping download.\n",
      "Downloading yellow_tripdata_2020-12.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2020-12.parquet...\n",
      "File yellow_tripdata_2020-12.parquet already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TAXI_URL: str = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "# Date range for filtering\n",
    "START_DATE = datetime(2020, 1, 1)\n",
    "END_DATE = datetime(2024, 8, 30)\n",
    "\n",
    "def get_taxi_html() -> str:\n",
    "    \"\"\"Fetch the HTML content of the taxi data page.\"\"\"\n",
    "    response = requests.get(TAXI_URL)\n",
    "    response.raise_for_status()\n",
    "    html = response.content\n",
    "    return html\n",
    "\n",
    "def find_taxi_parquet_links() -> List[str]:\n",
    "    \"\"\"Find links to Yellow Taxi and HVFHV Parquet files within the date range.\"\"\"\n",
    "    html = get_taxi_html()\n",
    "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # Find all <a> tags with relevant titles\n",
    "    yellow_a_tags = soup.find_all(\"a\", attrs={\"title\": \"Yellow Taxi Trip Records\"})\n",
    " \n",
    "    \n",
    "    # Combine all links\n",
    "    all_a_tags = yellow_a_tags\n",
    "    \n",
    "    # Extract href attributes and filter based on .parquet\n",
    "    parquet_links = [a[\"href\"].strip() for a in all_a_tags if \".parquet\" in (a.get(\"href\") or \"\")]\n",
    "    return filter_links_by_date(parquet_links)\n",
    "\n",
    "def filter_links_by_date(links: List[str]) -> List[str]:\n",
    "    \"\"\"Filter Parquet file links by date, retaining only those within the specified range.\"\"\"\n",
    "    filtered_links = []\n",
    "    date_pattern = re.compile(r\"_(\\d{4})-(\\d{2})\\.parquet\")\n",
    "    \n",
    "    for link in links:\n",
    "        match = date_pattern.search(link)\n",
    "        if match:\n",
    "            year, month = int(match.group(1)), int(match.group(2))\n",
    "            file_date = datetime(year, month, 1)\n",
    "            if START_DATE <= file_date <= END_DATE:\n",
    "                filtered_links.append(link)\n",
    "    \n",
    "    return filtered_links\n",
    "\n",
    "def download_files(links: List[str], folder_name: str) -> None:\n",
    "    \"\"\"Download files from a list of links and save them to the specified folder.\"\"\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "        \n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\"}\n",
    "    \n",
    "    for link in links:\n",
    "        file_name = link.split(\"/\")[-1]\n",
    "        file_path = os.path.join(folder_name, file_name)\n",
    "        print(f\"Downloading {file_name} from {link}...\")\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"File {file_name} already exists. Skipping download.\")\n",
    "            continue  # Skip to the next file if it already exists\n",
    "        \n",
    "        print(f\"Downloading {file_name} from {link}...\")\n",
    "        \n",
    "        # Request with headers to mimic a browser\n",
    "        response = requests.get(link, headers=headers)\n",
    "        response.raise_for_status()  # Check if download was successful\n",
    "        \n",
    "        with open(file_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded {file_name}\")\n",
    "\n",
    "# Find and download filtered links\n",
    "filtered_links = find_taxi_parquet_links()\n",
    "download_files(filtered_links, \"yellow_taxi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912745bb-3b0b-4726-86b1-0fc9a0a05f63",
   "metadata": {},
   "source": [
    "### COMMON FUNCTION: Dowloading the HVFHV Parquet links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading fhvhv_tripdata_2024-01.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet...\n",
      "File fhvhv_tripdata_2024-01.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2024-02.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet...\n",
      "File fhvhv_tripdata_2024-02.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2024-03.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet...\n",
      "File fhvhv_tripdata_2024-03.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2024-04.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-04.parquet...\n",
      "File fhvhv_tripdata_2024-04.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2024-05.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-05.parquet...\n",
      "File fhvhv_tripdata_2024-05.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2024-06.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-06.parquet...\n",
      "File fhvhv_tripdata_2024-06.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2024-07.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-07.parquet...\n",
      "File fhvhv_tripdata_2024-07.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2024-08.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-08.parquet...\n",
      "File fhvhv_tripdata_2024-08.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2023-01.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-01.parquet...\n",
      "File fhvhv_tripdata_2023-01.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2023-02.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-02.parquet...\n",
      "File fhvhv_tripdata_2023-02.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2023-03.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-03.parquet...\n",
      "File fhvhv_tripdata_2023-03.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2023-04.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-04.parquet...\n",
      "File fhvhv_tripdata_2023-04.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2023-05.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-05.parquet...\n",
      "File fhvhv_tripdata_2023-05.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2023-06.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-06.parquet...\n",
      "File fhvhv_tripdata_2023-06.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2023-07.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-07.parquet...\n",
      "File fhvhv_tripdata_2023-07.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2023-08.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-08.parquet...\n",
      "File fhvhv_tripdata_2023-08.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2023-09.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-09.parquet...\n",
      "File fhvhv_tripdata_2023-09.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2023-10.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-10.parquet...\n",
      "File fhvhv_tripdata_2023-10.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2023-11.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-11.parquet...\n",
      "File fhvhv_tripdata_2023-11.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2023-12.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-12.parquet...\n",
      "File fhvhv_tripdata_2023-12.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2022-01.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-01.parquet...\n",
      "File fhvhv_tripdata_2022-01.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2022-02.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-02.parquet...\n",
      "File fhvhv_tripdata_2022-02.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2022-03.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-03.parquet...\n",
      "File fhvhv_tripdata_2022-03.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2022-04.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-04.parquet...\n",
      "File fhvhv_tripdata_2022-04.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2022-05.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-05.parquet...\n",
      "File fhvhv_tripdata_2022-05.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2022-06.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-06.parquet...\n",
      "File fhvhv_tripdata_2022-06.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2022-07.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-07.parquet...\n",
      "File fhvhv_tripdata_2022-07.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2022-08.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-08.parquet...\n",
      "File fhvhv_tripdata_2022-08.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2022-09.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-09.parquet...\n",
      "File fhvhv_tripdata_2022-09.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2022-10.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-10.parquet...\n",
      "File fhvhv_tripdata_2022-10.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2022-11.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-11.parquet...\n",
      "File fhvhv_tripdata_2022-11.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2022-12.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2022-12.parquet...\n",
      "File fhvhv_tripdata_2022-12.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2021-01.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-01.parquet...\n",
      "File fhvhv_tripdata_2021-01.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2021-02.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-02.parquet...\n",
      "File fhvhv_tripdata_2021-02.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2021-03.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-03.parquet...\n",
      "File fhvhv_tripdata_2021-03.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2021-04.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-04.parquet...\n",
      "File fhvhv_tripdata_2021-04.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2021-05.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-05.parquet...\n",
      "File fhvhv_tripdata_2021-05.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2021-06.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-06.parquet...\n",
      "File fhvhv_tripdata_2021-06.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2021-07.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-07.parquet...\n",
      "File fhvhv_tripdata_2021-07.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2021-08.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-08.parquet...\n",
      "File fhvhv_tripdata_2021-08.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2021-09.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-09.parquet...\n",
      "File fhvhv_tripdata_2021-09.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2021-10.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-10.parquet...\n",
      "File fhvhv_tripdata_2021-10.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2021-11.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-11.parquet...\n",
      "File fhvhv_tripdata_2021-11.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2021-12.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-12.parquet...\n",
      "File fhvhv_tripdata_2021-12.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2020-01.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-01.parquet...\n",
      "File fhvhv_tripdata_2020-01.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2020-02.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-02.parquet...\n",
      "File fhvhv_tripdata_2020-02.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2020-03.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-03.parquet...\n",
      "File fhvhv_tripdata_2020-03.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2020-04.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-04.parquet...\n",
      "File fhvhv_tripdata_2020-04.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2020-05.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-05.parquet...\n",
      "File fhvhv_tripdata_2020-05.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2020-06.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-06.parquet...\n",
      "File fhvhv_tripdata_2020-06.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2020-07.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-07.parquet...\n",
      "File fhvhv_tripdata_2020-07.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2020-08.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-08.parquet...\n",
      "File fhvhv_tripdata_2020-08.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2020-09.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-09.parquet...\n",
      "File fhvhv_tripdata_2020-09.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2020-10.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-10.parquet...\n",
      "File fhvhv_tripdata_2020-10.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2020-11.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-11.parquet...\n",
      "File fhvhv_tripdata_2020-11.parquet already exists. Skipping download.\n",
      "Downloading fhvhv_tripdata_2020-12.parquet from https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2020-12.parquet...\n",
      "File fhvhv_tripdata_2020-12.parquet already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "def get_all_urls_from_tlc_page() -> str:\n",
    "    \"\"\"Fetch the HTML content of the taxi data page.\"\"\"\n",
    "    response = requests.get(TAXI_URL)\n",
    "    response.raise_for_status()  # Raise an error for bad responses\n",
    "    html = response.content\n",
    "    return html\n",
    "TAXI_URL: str = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "# Date range for filtering\n",
    "START_DATE = datetime(2020, 1, 1)\n",
    "END_DATE = datetime(2024, 8, 30)\n",
    "\n",
    "def get_taxi_html() -> str:\n",
    "    \"\"\"Fetch the HTML content of the taxi data page.\"\"\"\n",
    "    response = requests.get(TAXI_URL)\n",
    "    response.raise_for_status()\n",
    "    html = response.content\n",
    "    return html\n",
    "\n",
    "def find_taxi_parquet_links() -> List[str]:\n",
    "    \"\"\"Find links to Yellow Taxi and HVFHV Parquet files within the date range.\"\"\"\n",
    "    html = get_taxi_html()\n",
    "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # Find all <a> tags with relevant titles\n",
    "    hvfhv_a_tags = soup.find_all(\"a\", href=re.compile(r\"fhvhv.*\\.parquet\", re.IGNORECASE))\n",
    " \n",
    "    \n",
    "    # Combine all links\n",
    "    all_a_tags = hvfhv_a_tags\n",
    "    \n",
    "    # Extract href attributes and filter based on .parquet\n",
    "    parquet_links = [a[\"href\"].strip() for a in all_a_tags if \".parquet\" in (a.get(\"href\") or \"\")]\n",
    "    return filter_links_by_date(parquet_links)\n",
    "\n",
    "def filter_links_by_date(links: List[str]) -> List[str]:\n",
    "    \"\"\"Filter Parquet file links by date, retaining only those within the specified range.\"\"\"\n",
    "    filtered_links = []\n",
    "    date_pattern = re.compile(r\"_(\\d{4})-(\\d{2})\\.parquet\")\n",
    "    \n",
    "    for link in links:\n",
    "        match = date_pattern.search(link)\n",
    "        if match:\n",
    "            year, month = int(match.group(1)), int(match.group(2))\n",
    "            file_date = datetime(year, month, 1)\n",
    "            if START_DATE <= file_date <= END_DATE:\n",
    "                filtered_links.append(link)\n",
    "    \n",
    "    return filtered_links\n",
    "\n",
    "def download_files(links: List[str], folder_name: str) -> None:\n",
    "    \"\"\"Download files from a list of links and save them to the specified folder.\"\"\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "        \n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\"}\n",
    "    \n",
    "    for link in links:\n",
    "        file_name = link.split(\"/\")[-1]\n",
    "        file_path = os.path.join(folder_name, file_name)\n",
    "        print(f\"Downloading {file_name} from {link}...\")\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"File {file_name} already exists. Skipping download.\")\n",
    "            continue  # Skip to the next file if it already exists\n",
    "        \n",
    "        print(f\"Downloading {file_name} from {link}...\")\n",
    "        \n",
    "        # Request with headers to mimic a browser\n",
    "        response = requests.get(link, headers=headers)\n",
    "        response.raise_for_status()  # Check if download was successful\n",
    "        \n",
    "        with open(file_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded {file_name}\")\n",
    "\n",
    "# Find and download filtered links\n",
    "filtered_links = find_taxi_parquet_links()\n",
    "download_files(filtered_links, \"fhvhv_raw\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4528f34e-a2e4-48bb-b2f3-9ac553a66de9",
   "metadata": {},
   "source": [
    "### FILTERING ONLY UBER DATA from FHVHV parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb898a-36b9-42c7-8c78-f5cb762d6cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0daef487-9a38-4335-b63c-c00e6dda3cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data' already exists.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2020-01.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2020-01.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2020-02.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2020-02.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2020-03.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2020-03.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2020-04.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2020-04.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2020-05.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2020-05.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2020-06.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2020-06.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2020-07.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2020-07.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2020-08.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2020-08.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2020-09.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2020-09.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2020-10.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2020-10.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2020-11.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2020-11.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2020-12.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2020-12.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2021-01.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2021-01.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2021-02.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2021-02.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2021-03.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2021-03.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2021-04.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2021-04.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2021-05.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2021-05.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2021-06.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2021-06.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2021-07.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2021-07.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2021-08.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2021-08.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2021-09.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2021-09.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2021-10.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2021-10.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2021-11.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2021-11.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2021-12.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2021-12.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2022-01.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2022-01.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2022-02.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2022-02.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2022-03.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2022-03.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2022-04.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2022-04.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2022-05.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2022-05.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2022-06.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2022-06.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2022-07.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2022-07.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2022-08.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2022-08.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2022-09.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2022-09.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2022-10.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2022-10.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2022-11.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2022-11.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2022-12.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2022-12.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2023-01.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2023-01.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2023-02.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2023-02.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2023-03.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2023-03.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2023-04.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2023-04.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2023-05.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2023-05.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2023-06.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2023-06.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2023-07.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2023-07.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2023-08.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2023-08.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2023-09.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2023-09.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2023-10.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2023-10.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2023-11.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2023-11.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2023-12.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2023-12.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2024-01.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2024-01.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2024-02.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2024-02.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2024-03.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2024-03.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2024-04.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2024-04.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2024-05.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2024-05.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2024-06.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2024-06.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2024-07.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2024-07.parquet.\n",
      "Output file C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\\uber__tripdata_2024-08.parquet already exists. Skipping processing for C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\\fhvhv_tripdata_2024-08.parquet.\n"
     ]
    }
   ],
   "source": [
    "source_folder = r\"C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\fhvhv_raw\"\n",
    "output_folder = r\"C:\\Users\\sanch\\TOOLS for Analytics\\Project 1\\processed_uber_data\"\n",
    "\n",
    "# Ensure the output folder exists\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "    print(f\"Created output folder: {output_folder}\")\n",
    "else:\n",
    "    print(f\"The folder '{output_folder}' already exists.\")\n",
    "\n",
    "# List all Parquet files in the source folder\n",
    "files = [os.path.join(source_folder, f) for f in os.listdir(source_folder) if f.endswith('.parquet')]\n",
    "\n",
    "# Process each file\n",
    "for file_path in files:\n",
    "    # Extract the part of the filename after \"fhvhv\" to create the new file name\n",
    "    base_name = os.path.basename(file_path)  # Extracts the filename with extension\n",
    "    new_file_name = f\"uber_{base_name.split('fhvhv')[-1]}\"  # Create new filename with \"uber_\" prefix\n",
    "    output_file_path = os.path.join(output_folder, new_file_name)  # Full output path\n",
    "\n",
    "    # Check if the output file already exists\n",
    "    if os.path.exists(output_file_path):\n",
    "        print(f\"Output file {output_file_path} already exists. Skipping processing for {file_path}.\")\n",
    "        continue\n",
    "\n",
    "    # Read the Parquet file into a DataFrame\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # Filter the rows where 'hvfhs_license_num' is 'HV0003'\n",
    "    filtered_df = df[df['hvfhs_license_num'] == 'HV0003']\n",
    "\n",
    "    # Save the filtered DataFrame to a new Parquet file\n",
    "    filtered_df.to_parquet(output_file_path)\n",
    "\n",
    "    print(f\"Filtered data from {file_path} saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69366cf-3005-45dc-954d-8655d113efb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e754ac-8982-4418-ad8f-aba68d15733b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c3087-b6b7-4cae-a8c4-a10fbd783237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_month(url):    #### QUESTION?: IS it after SAMPLING each month's data \n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls):\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.contact(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    all_urls = get_all_urls_from_taxi_page(TLC_URL)\n",
    "    all_parquet_urls = find_taxi_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07574983-f41d-4cd6-8f70-489493089b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_month(url):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d85ff-313c-41a2-9a46-261a9a2bb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_data(parquet_urls):\n",
    "    all_uber_dataframes = []\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_uber_month(parquet_url)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_uber_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    uber_data = pd.contact(all_uber_dataframes)\n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data():\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    all_urls = get_all_urls_from_tlc_page(TLC_URL)\n",
    "    all_parquet_urls = find_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_uber_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d783db-e527-4847-bf70-2d7428ea3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddeb14-cd70-4e83-8f93-974642c3bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935261b7-ae23-427c-97ff-ea31aa4e44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcb502-d1d1-447d-aa68-11bff0dc53b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090eb94-a5b0-4d93-bf82-a596d2521b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c074aa3-a5f2-4586-8748-411e1e6c11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
